# Paper List in the survey paper
The papers that we are surveying are listed in this file. The papers are grouped by the following categories:
- [Foundation models used in Robotics](#foundation-models-used-in-robotics). For these papers, the authors apply existing foundation models, such as LLM, VLM, vision FM and text-conditioned image generation models in modules of robotics, such as perception, decision making and planning, and action.

- [Robotic Foundation Models](#robotic-foundation-models). For these papers, the authors propose new foundation models used in one specific robotic applications, such as control using imilation learning and reinforcement learning. We also include genera-purpose foundation models, such as GATO, PALM-E in this category.

The taxonomy is shown in this figure,
<p align="center">
<img width="600" src="./assets/taxonomy3.jpg"/>
</p>

## Foundation models used in Robotics

### Perception
- CLIPORT **CLIPORT: What and Where Pathways for Robotic Manipulation**, 2021, [paper link](https://arxiv.org/pdf/2109.12098.pdf)
- LM-Nav **LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action**, 2022, [paper link](https://arxiv.org/pdf/2207.04429.pdf)
- NLMap **Open-vocabulary Queryable Scene Representations for Real World Planning**, 2022, [Paper Link](https://arxiv.org/pdf/2209.09874.pdf)
- CLIP-Fields **CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory**, 2022, [paper link](https://arxiv.org/abs/2210.05663)
- VLMap **Visual Language Maps for Robot Navigation**, 2022, [paper link](https://arxiv.org/pdf/2210.05714.pdf)
- ConceptFusion **ConceptFusion: Open-set Multimodal 3D Mapping**, 2023, [Paper Link](https://arxiv.org/pdf/2302.07241.pdf)
- ROSIE **Scaling Robot Learning with Semantically Imagined Experience**, 2023, [paper link](https://arxiv.org/pdf/2302.11550.pdf)
- AnyLoc **Towards Universal Visual Place Recognition**, 2023, [paper link](https://anyloc.github.io/assets/AnyLoc.pdf)
- WVN **Fast Traversability Estimation for Wild Visual Navigation**, 2023, [paper link](https://arxiv.org/pdf/2305.08510.pdf)
- F3RM **Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation**, 2023, [paper link](https://arxiv.org/pdf/2308.07931.pdf)


### Decision Making and Planning
<!-- #### Task Planning -->
<!-- - **Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents**, 2022 [paper link](https://arxiv.org/pdf/2201.07207.pdf) -->
- **Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers**, 2022, [paper link](https://arxiv.org/pdf/2203.13411.pdf)
- SayCan **Do As I Can, Not As I Say: Grounding Language in Robotic Affordances**, 2022, [paper link](https://arxiv.org/pdf/2204.01691.pdf)
- **Correcting Robot Plans with Natural Language Feedback**, 2022, [paper link](https://arxiv.org/pdf/2204.05186.pdf)
- Housekeep **Housekeep: Tidying Virtual Households using Commonsense Reasoning**, 2022, [paper link](https://arxiv.org/pdf/2205.10712.pdf)
- Inner Monologue **Inner Monologue: Embodied Reasoning through Planning with Language Models**, 2022, [paper link](https://arxiv.org/pdf/2207.05608.pdf)
- Code as Policies **Code as Policies: Language Model Programs for Embodied Control**, 2022, [paper link](https://arxiv.org/pdf/2209.07753.pdf)
- ProgPrompt **ProgPrompt: Generating Situated Robot Task Plans using Large Language Models**, 2022, [paper link](https://arxiv.org/abs/2209.11302)
- VIMA **VIMA: General Robot Manipulation with Multimodal Prompts**, 2022, [paper link](https://arxiv.org/pdf/2210.03094.pdf)
- LILAC **“No, to the Right” – Online Language Corrections for Robotic Manipulation via Shared Autonomy**, 2023, [paper link](https://arxiv.org/pdf/2301.02555.pdf)
<!-- - **Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents**, 2023, [paper link](https://arxiv.org/pdf/2302.01560.pdf) -->
- ChatGPT for Robotics **ChatGPT for Robotics: Design Principles and Model Abilities**, 2023, [paper link](https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf)
- Grounded Decoding **Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control**, 2023, [paper link](https://arxiv.org/pdf/2303.00855.pdf)
- Socratic Models **Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language**, 2023, [paper link](https://arxiv.org/abs/2204.00598)
- TidyBot **TidyBot: Personalized Robot Assistance with Large Language Models**, 2023, [paper link](https://arxiv.org/pdf/2305.05658.pdf)
- Instruct2Act **Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model**, 2023, [paper link](https://arxiv.org/pdf/2305.11176.pdf)
- VoxPoser **VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models**, 2023, [paper link](https://voxposer.github.io/voxposer.pdf)
- KNOWNO **Robots That Ask For Help:Uncertainty Alignment for Large Language Model Planners**, 2023, [paper link](https://arxiv.org/pdf/2307.01928.pdf)
- SayPlan **SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning**, 2023, [paper link](https://arxiv.org/pdf/2307.06135.pdf)

### Action
- SayTap **SayTap: Language to Quadrupedal Locomotion**, 2023, [paper link](https://arxiv.org/pdf/2306.07580.pdf)
- L2R **Language to Rewards for Robotic Skill Synthesis**, 2023, [Paper Link](https://arxiv.org/pdf/2306.08647.pdf)


## Robotic Foundation Models

### Modular

#### Decision-making and Planning 
- GNM **GNM: A General Navigation Model to Drive Any Robot**, 2022, [Paper Link](https://arxiv.org/pdf/2210.03370.pdf)
- STAP **STAP: Sequencing Task-Agnostic Policies**, 2022, [paper link](https://arxiv.org/abs/2210.12250)
- ViNT **ViNT: A Foundation Model for Visual Navigation**, 2023, [Paper Link](https://arxiv.org/pdf/2306.14846.pdf)

#### Action 
<!-- - **Pre-Trained Language Models for Interactive Decision-Making**, 2022, [paper link](https://arxiv.org/pdf/2202.01771.pdf) -->
- ZeST **Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?**, 2022, [paper link](https://arxiv.org/pdf/2204.11134.pdf)
- ATLA **Leveraging Language for Accelerated Learning of Tool Manipulation**, 2022, [paper link](https://arxiv.org/pdf/2206.13074.pdf)
- LATTE **LATTE: LAnguage Trajectory TransformEr**, 2022, [paper link](https://arxiv.org/pdf/2208.02918.pdf)
- Perceiver-Actor **Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation**, 2022, [paper link](https://arxiv.org/pdf/2209.05451.pdf)
- **Real-World Robot Learning with Masked Visual Pre-training**, 2022, [paper link](https://arxiv.org/pdf/2210.03109.pdf)
- Interactive Language **Interactive Language: Talking to Robots in Real Time**, 2022, [paper link](https://arxiv.org/pdf/2210.06407.pdf)
- LILA **LILA: Language-Informed Latent Actions**, 2022, [paper link](https://arxiv.org/pdf/2111.03205.pdf)
- DIAL **Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models**, 2022, [paper link](https://arxiv.org/pdf/2211.11736.pdf)
- RT-1 **RT-1: Robotics Transformer for Real-World Control at Scale**, 2022, [paper link](https://arxiv.org/pdf/2212.06817.pdf)
- MOO **Open-World Object Manipulation using Pre-Trained Vision-Language Models**, 2023, [paper link](https://arxiv.org/pdf/2303.00905.pdf)
- RC-1 **Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?**, 2023, [paper link](https://arxiv.org/pdf/2303.18240.pdf)
- CoTPC **Chain-of-Thought Predictive Control**, 2023, [paper link] (https://arxiv.org/pdf/2304.00776.pdf)
- Optimus **Optimus: TAMP-supervised visuomotor transformers**, 2023, [paper link](https://arxiv.org/pdf/2305.16309v1.pdf)
- RoboCat **RoboCat: A self-improving robotic agent**, 2023, [paper link](https://arxiv.org/pdf/2306.11706.pdf)
- RT-2 **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control**, 2023, [paper link](https://robotics-transformer2.github.io/assets/rt2.pdf)
- Scaling Up and Distilling Down **Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition**, 2023, [paper link](https://arxiv.org/pdf/2307.14535.pdf)

### Unified
- GATO **A Generalist Agent**, 2022, [paper link](https://arxiv.org/pdf/2205.06175.pdf)
- PACT **PACT: Perception-Action Causal Transformer for Autoregressive Robotics Pre-Training**, 2022, [paper link](https://arxiv.org/pdf/2209.11133.pdf)
- PALM-E **PaLM-E: An Embodied Multimodal Language Model**, 2023, [paper link](https://arxiv.org/pdf/2303.03378.pdf)


## Related Surveys and repositories
- **Reinforcement Learning in Robotics: A Survey**, 2013, [paper link](https://www.ri.cmu.edu/pub_files/2013/7/Kober_IJRR_2013.pdf)
- **A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms**, 2021, [paper link](https://www.jmlr.org/papers/volume22/19-804/19-804.pdf)
- **On the Opportunities and Risks of Foundation Models**, 2021, [paper link](https://arxiv.org/pdf/2303.04129.pdf)
- **Foundation Models for Decision Making: Problems, Methods, and Opportunities**, 2023, [paper link](https://arxiv.org/pdf/2301.02555.pdf)
- **Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond**, 2023, [paper link](https://arxiv.org/pdf/2304.13712.pdf)
- **Challenges and Applications of Large Language Models**, 2023, [paper link](https://arxiv.org/pdf/2307.10169.pdf)
- **Awesome-LLM-Robotics** [repo link](https://github.com/GT-RIPL/Awesome-LLM-Robotics)


