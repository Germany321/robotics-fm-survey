# Paper List in the survey paper
The papers that we are surveying are listed in this file. The papers are grouped by the following categories:
- [Foundation models used in Robotics](#foundation-models-used-in-robotics). For these papers, the authors apply existing foundation models, such as LLM, VLM, vision FM and text-conditioned image generation models in modules of robotics, such as perception, decision making and planning, and action.

- [Robotic Foundation Models](#robotic-foundation-models). For these papers, the authors propose new foundation models used in one specific robotic applications, such as control using imilation learning and reinforcement learning. We also include genera-purpose foundation models, such as GATO, PALM-E in this category.

## Foundation models used in Robotics

### Perception
- CLIPORT **CLIPORT: What and Where Pathways for Robotic Manipulation**, 2021, [paper link](https://arxiv.org/pdf/2109.12098.pdf)
- LM-Nav **LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action**, 2022, [paper link](https://arxiv.org/pdf/2207.04429.pdf)
- NLMap**Open-vocabulary Queryable Scene Representations for Real World Planning**, 2022, [Paper Link](https://arxiv.org/pdf/2209.09874.pdf)
- CLIP-Fields **CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory**, 2022, [paper link](https://arxiv.org/abs/2210.05663)
- VLMap **Visual Language Maps for Robot Navigation**, 2022, [paper link](https://arxiv.org/pdf/2210.05714.pdf)
- ConceptFusion **ConceptFusion: Open-set Multimodal 3D Mapping**, 2023, [Paper Link](https://arxiv.org/pdf/2302.07241.pdf)
- ROSIE **Scaling Robot Learning with Semantically Imagined Experience**, 2023, [paper link](https://arxiv.org/pdf/2302.11550.pdf)

### Decision Making and Planning
#### Task Planning
- **Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents**, 2022 [paper link](https://arxiv.org/pdf/2201.07207.pdf)
- **Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers**, 2022, [paper link](https://arxiv.org/pdf/2203.13411.pdf)
- SayCan **Do As I Can, Not As I Say: Grounding Language in Robotic Affordances**, 2022, [paper link](https://arxiv.org/pdf/2204.01691.pdf)
- **Correcting Robot Plans with Natural Language Feedback**, 2022, [paper link](https://arxiv.org/pdf/2204.05186.pdf)
- Housekeep **Housekeep: Tidying Virtual Households using Commonsense Reasoning**, 2022, [paper link](https://arxiv.org/pdf/2205.10712.pdf)
- Inner Monologue **Inner Monologue: Embodied Reasoning through Planning with Language Models**, 2022, [paper link](https://arxiv.org/pdf/2207.05608.pdf)
- Code as Policies **Code as Policies: Language Model Programs for Embodied Control**, 2022, [paper link](https://arxiv.org/pdf/2209.07753.pdf)
- ProgPrompt **ProgPrompt: Generating Situated Robot Task Plans using Large Language Models**, 2022, [paper link](https://arxiv.org/abs/2209.11302)
- VIMA **VIMA: General Robot Manipulation with Multimodal Prompts**, 2022, [paper link](https://arxiv.org/pdf/2210.03094.pdf)
- **Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models**, 2022, [paper link](https://arxiv.org/pdf/2211.11736.pdf)
- **“No, to the Right” – Online Language Corrections for Robotic Manipulation via Shared Autonomy**, 2023, [paper link](https://arxiv.org/pdf/2301.02555.pdf)
- **Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents**, 2023, [paper link](https://arxiv.org/pdf/2302.01560.pdf)
- Grounded Decoding **Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control**, 2023, [paper link](https://arxiv.org/pdf/2303.00855.pdf)
- ChatGPT for Robotics **ChatGPT for Robotics: Design Principles and Model Abilities**, 2023, [paper link](https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf)
- Socratic Models **Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language**, 2023, [paper link](https://arxiv.org/abs/2204.00598)
- TidyBot **TidyBot: Personalized Robot Assistance with Large Language Models**, 2023, [paper link](https://arxiv.org/pdf/2305.05658.pdf)

### Action
- L2R **Language to Rewards for Robotic Skill Synthesis**, 2021, [Paper Link](https://arxiv.org/pdf/2306.08647.pdf)


## Robotic Foundation Models

### Single-purpose 

#### Action 
- **Pre-Trained Language Models for Interactive Decision-Making**, 2022, [paper link](https://arxiv.org/pdf/2202.01771.pdf)
- **Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?**, 2022, [paper link](https://arxiv.org/pdf/2204.11134.pdf)
- **Real-World Robot Learning with Masked Visual Pre-training**, 2022, [paper link](https://arxiv.org/pdf/2210.03109.pdf)
- Interactive Language **Interactive Language: Talking to Robots in Real Time**, 2022, [paper link](https://arxiv.org/pdf/2210.06407.pdf)
- LILA **LILA: Language-Informed Latent Actions**, 2022, [paper link](https://arxiv.org/pdf/2111.03205.pdf)
- RT-1 **RT-1: Robotics Transformer for Real-World Control at Scale**, 2022, [paper link](https://arxiv.org/pdf/2212.06817.pdf)
- **Open-World Object Manipulation using Pre-Trained Vision-Language Models**, 2023, [paper link](https://arxiv.org/pdf/2303.00905.pdf)
- RC-1 **Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?**, 2023, [paper link](https://arxiv.org/pdf/2303.18240.pdf)
- **Chain-of-Thought Predictive Control**, 2023, [paper link] (https://arxiv.org/pdf/2304.00776.pdf)
- Optimus **Optimus: TAMP-supervised visuomotor transformers**, 2023, [paper link] (https://arxiv.org/pdf/2305.16309v1.pdf)

#### Decision-making and Planning 
- GNM **GNM: A General Navigation Model to Drive Any Robot**, 2022, [Paper Link](https://arxiv.org/pdf/2210.03370.pdf)

### General-purpose
- GATO **A Generalist Agent**, 2021, [paper link](https://arxiv.org/pdf/2205.06175.pdf)
- PALM-E **PaLM-E: An Embodied Multimodal Language Model**, 2023, [paper link](https://arxiv.org/pdf/2303.03378.pdf

